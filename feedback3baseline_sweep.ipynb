{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SUPER-TADORY/-/blob/main/feedback3baseline_sweep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYV0w87rMLlH"
      },
      "source": [
        "#初期操作"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "junwO31jj8Kj"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mvd-SJt0kB5g"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "import os\n",
        "print(os.cpu_count())\n",
        "#kaggle APIキーをupload\n",
        "# #kaggle APIキーをupload\n",
        "if os.path.exists(\"/content/drive/MyDrive/Kaggle/competitions/\"):\n",
        "    !cp /content/drive/MyDrive/Kaggle/utils/kaggle.json ./\n",
        "else:\n",
        "    from google.colab import files\n",
        "    files.upload()\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zP6jqrfjkEhR"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle > /dev/null 2>&1\n",
        "!pip install --target ./python requests google-api-python-client google-auth requests-oauthlib > /dev/null 2>&1\n",
        "!pip install madgrad > /dev/null 2>&1\n",
        "!pip install transformers > /dev/null 2>&1\n",
        "!pip install sentencepiece > /dev/null 2>&1\n",
        "!pip install wandb > /dev/null 2>&1\n",
        "!pip install iterative-stratification\n",
        "!python3 -m textblob.download_corpora\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "import io, os\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "\n",
        "drive_service = build('drive', 'v3')\n",
        "results = drive_service.files().list(\n",
        "        q=\"name = 'kaggle.json'\", fields=\"files(id)\").execute()\n",
        "kaggle_api_key = results.get('files', [])\n",
        "\n",
        "filename = \"/root/.kaggle/kaggle.json\"\n",
        "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b554lHIdkR5g"
      },
      "outputs": [],
      "source": [
        "!mkdir /kaggle\n",
        "!kaggle competitions download -c feedback-prize-english-language-learning -p /kaggle/\n",
        "!unzip -q /kaggle/feedback-prize-english-language-learning.zip -d /kaggle\n",
        "!rm /kaggle/feedback-prize-english-language-learning.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSl_HKxKK5d4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCys3OThXTu0"
      },
      "source": [
        "# ディレクトリの作成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HiBm7LwvuV8"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# if os.path.exists(\"/content/drive/MyDrive/Kaggle/competitions/Feedback_Prize_Predicting_Effective_Arguments/code\"):\n",
        "#   ROOT_PATH = \"/content/drive/MyDrive/Kaggle/competitions/Feedback_Prize_Predicting_Effective_Arguments/code/feedbackprize\"\n",
        "# else:\n",
        "\n",
        "#drive\n",
        "ROOT_DRIVEPATH = \"/content/drive/MyDrive/results_for_furu\"\n",
        "\n",
        "#Local\n",
        "ROOT_PATH = \"/kaggle\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tY67PAjLXU1X"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import datetime\n",
        "# For descriptive error messages\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "dt_now = datetime.datetime.now()\n",
        "savename = dt_now.strftime('%Y-%m-%d')\n",
        "version = f\"{savename}_opt_fold[2,3]\"\n",
        "savedir = f\"{ROOT_PATH}/{version}\"\n",
        "savedir_drive = f\"{ROOT_DRIVEPATH}/{version}\"\n",
        "\n",
        "# local fold\n",
        "os.makedirs(savedir,exist_ok=True)\n",
        "srcdir = savedir + \"/src\"\n",
        "os.makedirs(srcdir,exist_ok=True)\n",
        "\n",
        "# drive fold\n",
        "os.makedirs(savedir_drive,exist_ok=True)\n",
        "srcdir_drive = savedir_drive + \"/src\"\n",
        "os.makedirs(srcdir_drive,exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ubj644oc3l7i"
      },
      "source": [
        "\n",
        "## srcの定義\n",
        "\n",
        "### 編集するときは writefileをコメントアウト(保存するときにコメントアウト外す)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fM6BM1ZPElk"
      },
      "source": [
        "## `src/__init__.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bt7H_JRIPEPq"
      },
      "outputs": [],
      "source": [
        "%%writefile {srcdir}/__init__.py\n",
        "\n",
        "from .models import FeedBackModel\n",
        "from .helper_func import helper\n",
        "from .trainer import Trainers\n",
        "from .train_datasets import FeedBackDataset, Collate\n",
        "#from .eval_datasets import essay_ds, discourse_ds, disc_collate, essay_collate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdrPHintUIT0"
      },
      "source": [
        "## src/models.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0wT51kM1S_c"
      },
      "outputs": [],
      "source": [
        "%%writefile {srcdir}/models.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
        "\n",
        "def freeze(module):\n",
        "    \"\"\"\n",
        "    Freezes module's parameters.\n",
        "    \"\"\"\n",
        "    \n",
        "    for parameter in module.parameters():\n",
        "        parameter.requires_grad = False\n",
        "\n",
        "class MeanPooling(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MeanPooling, self).__init__()\n",
        "        \n",
        "    def forward(self, last_hidden_state, attention_mask):\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
        "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
        "        sum_mask = input_mask_expanded.sum(1)\n",
        "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
        "        mean_embeddings = sum_embeddings / sum_mask\n",
        "        return mean_embeddings\n",
        "\n",
        "class FeedBackModel(nn.Module):\n",
        "    def __init__(self, model_name, opt_params=None, num_labels=6):\n",
        "        super(FeedBackModel, self).__init__()\n",
        "        self.config = AutoConfig.from_pretrained(model_name)\n",
        "        self.config.update(\n",
        "            {\n",
        "                \"hidden_dropout_prob\": 0.1,\n",
        "                \"layer_norm_eps\": 1e-5,\n",
        "                \"add_pooling_layer\": False,\n",
        "                \"num_labels\": num_labels,\n",
        "            }\n",
        "        )\n",
        "        if opt_params is not None:\n",
        "            self.config.update(\n",
        "                {\n",
        "                    \"hidden_dropout_prob\": opt_params[\"backbone_dropout_prob\"],\n",
        "                    \"layer_norm_eps\": opt_params[\"layer_norm_eps\"],\n",
        "                    \"add_pooling_layer\": False,\n",
        "                    \"attention_probs_dropout_prob\":0,\n",
        "                    \"num_labels\": num_labels,\n",
        "                }\n",
        "            )\n",
        "        \n",
        "        self.model = AutoModel.from_pretrained(model_name, config=self.config)\n",
        "        if opt_params is not None:\n",
        "            self.dropout = nn.Dropout(p=opt_params[\"head_dropout_prob\"])\n",
        "            self.drop1 = nn.Dropout(p=opt_params[\"stable_prob1\"])\n",
        "            self.drop2 = nn.Dropout(p=opt_params[\"stable_prob2\"])\n",
        "            self.drop3 = nn.Dropout(p=opt_params[\"stable_prob3\"])\n",
        "            self.drop4 = nn.Dropout(p=opt_params[\"stable_prob4\"])\n",
        "            self.drop5 = nn.Dropout(p=opt_params[\"stable_prob5\"])\n",
        "        else:\n",
        "            self.dropout = nn.Dropout(p=0.1)\n",
        "            self.drop1 = nn.Dropout(p=0.1)\n",
        "            self.drop2 = nn.Dropout(p=0.2)\n",
        "            self.drop3 = nn.Dropout(p=0.3)\n",
        "            self.drop4 = nn.Dropout(p=0.4)\n",
        "            self.drop5 = nn.Dropout(p=0.5)\n",
        "        self.pooler = MeanPooling()\n",
        "        self.fc = nn.Linear(self.config.hidden_size, self.config.num_labels)\n",
        "        self.loss = nn.MSELoss()\n",
        "        #self.loss = nn.SmoothL1Loss()\n",
        "\n",
        "        if opt_params[\"freeze_emb\"]:\n",
        "            freeze(self.model.embeddings)\n",
        "            \n",
        "        if opt_params[\"freeze_layernum\"] > 0:\n",
        "            num = opt_params[\"freeze_layernum\"]\n",
        "            freeze(self.model.encoder.layer[:num])\n",
        "\n",
        "        if opt_params[\"reinit_layernum\"] > 0:\n",
        "            rnum = opt_params[\"reinit_layernum\"]\n",
        "            self._init_weights(self.model.encoder.layer[-rnum:])\n",
        "        \n",
        "        if opt_params[\"init_head\"]:\n",
        "            self._init_weights(self.fc)\n",
        "\n",
        "        self.model.gradient_checkpointing_enable()\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "    \n",
        "    def mcrmse_fn(self, outputs, targets):\n",
        "        colwise_mse = torch.mean(torch.square(targets - outputs), dim=0)\n",
        "        loss = torch.mean(torch.sqrt(colwise_mse), dim=0)\n",
        "        return loss\n",
        "\n",
        "    def get_emb(self, ids, mask, target=None,text_id=None):\n",
        "        out = self.model(input_ids=ids,attention_mask=mask\n",
        "                         ,output_hidden_states=False)\n",
        "        out = self.pooler(out.last_hidden_state,mask)\n",
        "        outputs = self.fc(out)\n",
        "        return outputs, out\n",
        "        \n",
        "    def forward(self, ids, mask, target=None,text_id=None):        \n",
        "        out = self.model(input_ids=ids,attention_mask=mask\n",
        "                         ,output_hidden_states=False)\n",
        "        out = self.pooler(out.last_hidden_state,mask)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        outputs = self.fc(out)\n",
        "\n",
        "        if target is not None:\n",
        "          loss1 = self.loss(self.fc(self.drop1(out)), target) \n",
        "          loss2 = self.loss(self.fc(self.drop2(out)), target) \n",
        "          loss3 = self.loss(self.fc(self.drop3(out)), target) \n",
        "          loss4 = self.loss(self.fc(self.drop4(out)), target) \n",
        "          loss5 = self.loss(self.fc(self.drop5(out)), target) \n",
        "          loss = (loss1 + loss2 + loss3 + loss4 + loss5)/5\n",
        "          loss = torch.sqrt(loss)\n",
        "          return loss\n",
        "        else:\n",
        "          return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d27SBsVY6lx"
      },
      "source": [
        "## src/train_datasets.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOhIK_3vY5ik"
      },
      "outputs": [],
      "source": [
        "%%writefile {srcdir}/train_datasets.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "\n",
        "from text_unidecode import unidecode\n",
        "from typing import Dict, List, Tuple\n",
        "import codecs\n",
        "\n",
        "class FeedBackDataset(Dataset):\n",
        "    def __init__(self, df, config):\n",
        "        if config[\"text_encode\"]:\n",
        "            df['full_text'] = df['full_text'].apply(lambda x : self.resolve_encodings_and_normalize(x))\n",
        "        self.df = df\n",
        "        self.text = df['full_text'].values\n",
        "        self.text_id = df['text_id'].values\n",
        "        self.targets = df[['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar','conventions']].values\n",
        "        self.textlength = df[\"text_length\"].values\n",
        "        self.config = config\n",
        "        self.tokenizer = config[\"tokenizer\"]\n",
        "\n",
        "    def resolve_encodings_and_normalize(self, text: str) -> str:\n",
        "        def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n",
        "            return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n",
        "\n",
        "\n",
        "        def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n",
        "            return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n",
        "\n",
        "        # Register the encoding and decoding error handlers for `utf-8` and `cp1252`.\n",
        "        codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n",
        "        codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n",
        "        \"\"\"Resolve the encoding problems and normalize the abnormal characters.\"\"\"\n",
        "        text = (\n",
        "            text.encode(\"raw_unicode_escape\")\n",
        "            .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
        "            .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
        "            .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
        "        )\n",
        "        text = unidecode(text)\n",
        "        return text\n",
        "          \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        text = self.text[index]\n",
        "        text_id = self.text_id[index]\n",
        "        target = self.targets[index]\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "                        text,\n",
        "                        truncation=True,\n",
        "                        add_special_tokens=True,\n",
        "                        max_length = self.config[\"max_length\"],\n",
        "                        padding=False,\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "\n",
        "        return {\n",
        "            'text_id': text_id,\n",
        "            'ids': ids,\n",
        "            'mask': mask,\n",
        "            'target': target\n",
        "        }\n",
        "\n",
        "class Collate:\n",
        "    def __init__(self, config):\n",
        "        self.tokenizer = config[\"tokenizer\"]\n",
        "        self.dropout_prob = config[\"token_dropout_prob\"]\n",
        "        self.dropout_ratio = config[\"token_dropout_ratio\"]\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        output = dict()\n",
        "        for name in [\"ids\",\"mask\", \"target\",\"text_id\"]:\n",
        "          output[name] = [sample[name] for sample in batch]\n",
        "\n",
        "        # calculate max token length of this batch\n",
        "        batch_max = max([len(ids) for ids in output[\"ids\"]])\n",
        "\n",
        "        # add padding\n",
        "        output[\"ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"ids\"]]\n",
        "        output[\"mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"mask\"]]\n",
        "\n",
        "        # convert to tensors\n",
        "        for name in [\"ids\", \"mask\"]:\n",
        "          output[name] = torch.tensor(output[name], dtype=torch.long)\n",
        "        output[\"target\"] = torch.tensor(output[\"target\"], dtype=torch.float)\n",
        "\n",
        "        if (self.dropout_prob > 0)&(random.uniform(0,1) < self.dropout_prob):\n",
        "            output[\"ids\"] = self.torch_mask_tokens(output[\"ids\"])\n",
        "\n",
        "        return output\n",
        "    \n",
        "    def torch_mask_tokens(self, inputs, special_tokens_mask = None):\n",
        "        probability_matrix = torch.full(inputs.shape, self.dropout_ratio)\n",
        "        special_tokens_mask = [\n",
        "            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in inputs.clone().tolist()\n",
        "        ]\n",
        "        special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
        "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
        "        masked_indices = torch.bernoulli(probability_matrix).bool()\\\n",
        "\n",
        "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
        "        indices_replaced = torch.bernoulli(torch.full(inputs.shape, 0.8)).bool() & masked_indices\n",
        "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
        "\n",
        "        # 10% of the time, we replace masked input tokens with random word\n",
        "        indices_random = torch.bernoulli(torch.full(inputs.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
        "        random_words = torch.randint(len(self.tokenizer), inputs.shape, dtype=torch.long)\n",
        "        inputs[indices_random] = random_words[indices_random]\n",
        "\n",
        "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
        "        return inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OULq_thKgQX"
      },
      "source": [
        "## src/test_datasets.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIQCBguKKj9n"
      },
      "outputs": [],
      "source": [
        "%%writefile {srcdir}/test_datasets.py\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class feedbacktestDataset(Dataset):\n",
        "    def __init__(self, df, config):\n",
        "        self.df = df\n",
        "        self.text_id = df['text_id'].values\n",
        "        self.text = df['full_text'].values\n",
        "        self.config = config\n",
        "        self.tokenizer = config[\"tokenizer\"]\n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        text = self.text[index]\n",
        "        text_id = self.text_id[index]\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "                        text,\n",
        "                        truncation=True,\n",
        "                        add_special_tokens=True,\n",
        "                        max_length = self.config[\"max_length\"],\n",
        "                        padding=False,\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "\n",
        "        return {\n",
        "            'text_id': text_id,\n",
        "            'ids': ids,\n",
        "            'mask': mask\n",
        "        }\n",
        "\n",
        "class testCollate:\n",
        "    def __init__(self, config):\n",
        "        self.tokenizer = config[\"tokenizer\"]\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        output = dict()\n",
        "        for name in [\"text_id\", \"ids\",\"mask\"]:\n",
        "          output[name] = [sample[name] for sample in batch]\n",
        "\n",
        "        # calculate max token length of this batch\n",
        "        batch_max = max([len(ids) for ids in output[\"ids\"]])\n",
        "\n",
        "        # add padding\n",
        "        output[\"ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"ids\"]]\n",
        "        output[\"mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"mask\"]]\n",
        "\n",
        "        # convert to tensors\n",
        "        for name in [\"ids\", \"mask\"]:\n",
        "          output[name] = torch.tensor(output[name], dtype=torch.long)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tY0cxYRUUKBc"
      },
      "source": [
        "## src/helper_func.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yw2zs4YJN5Z_"
      },
      "outputs": [],
      "source": [
        "%%writefile {srcdir}/helper_func.py\n",
        "\n",
        "import os, gc, copy, time, random, string, joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from text_unidecode import unidecode\n",
        "from typing import Dict, List, Tuple\n",
        "import codecs\n",
        "from textblob import TextBlob\n",
        "\n",
        "from sklearn.model_selection import GroupKFold, KFold, StratifiedGroupKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "try:\n",
        "    from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "except:\n",
        "    print(\"not installed iterstrat\")\n",
        "\n",
        "from .train_datasets import FeedBackDataset, Collate\n",
        "\n",
        "class helper:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.set_seed(config['seed'])\n",
        "\n",
        "    def resolve_encodings_and_normalize(self, text: str) -> str:\n",
        "        def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n",
        "            return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n",
        "\n",
        "\n",
        "        def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n",
        "            return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n",
        "\n",
        "        # Register the encoding and decoding error handlers for `utf-8` and `cp1252`.\n",
        "        codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n",
        "        codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n",
        "        \"\"\"Resolve the encoding problems and normalize the abnormal characters.\"\"\"\n",
        "        text = (\n",
        "            text.encode(\"raw_unicode_escape\")\n",
        "            .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
        "            .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
        "            .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
        "        )\n",
        "        text = unidecode(text)\n",
        "        return text\n",
        "\n",
        "    def set_seed(self, seed=42):\n",
        "        '''Sets the seed of the entire notebook so results are the same every time we run.\n",
        "        This is for REPRODUCIBILITY.'''\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        # When running on the CuDNN backend, two further options must be set\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        # Set a fixed value for the hash seed\n",
        "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "    def prepare_loaders_splite_trainstep(self, df, fold):\n",
        "        collate_fn = Collate(self.config)\n",
        "        df_valid = df[df.kfold == fold].sort_values(\"text_length\").reset_index(drop=True)\n",
        "        valid_dataset = FeedBackDataset(df_valid, config=self.config)\n",
        "        valid_loader = DataLoader(\n",
        "            valid_dataset, \n",
        "            batch_size=self.config['valid_batch_size'], \n",
        "            collate_fn = collate_fn, \n",
        "            num_workers=os.cpu_count(), \n",
        "            pin_memory=True, \n",
        "            shuffle=False,\n",
        "            drop_last=False,\n",
        "        )\n",
        "\n",
        "        df_train = df[df.kfold != fold].reset_index(drop=True)\n",
        "        train_dataset = FeedBackDataset(df_train, config=self.config)\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset, \n",
        "            batch_size=self.config['train_batch_size'], \n",
        "            collate_fn = collate_fn, \n",
        "            num_workers=os.cpu_count(), \n",
        "            pin_memory=True, \n",
        "            shuffle=True,\n",
        "            drop_last=True,\n",
        "        )\n",
        "\n",
        "        return train_loader, valid_loader\n",
        "\n",
        "    def get_df(self):\n",
        "        df = pd.read_csv(self.config[\"train_df\"])\n",
        "        #df['full_text'] = df['full_text'].apply(lambda x : self.resolve_encodings_and_normalize(x).strip().lower())\n",
        "        df[\"text_length\"] = df.full_text.apply(lambda x: len(x.split()))\n",
        "\n",
        "        mskf = MultilabelStratifiedKFold(n_splits=self.config['n_fold'], shuffle=True, random_state=self.config[\"fold_seed\"])\n",
        "        labels = [\"cohesion\", \"syntax\", \"vocabulary\", \"phraseology\", \"grammar\", \"conventions\"]\n",
        "        for fold, ( _, val_) in enumerate(mskf.split(df, df[labels].values)):\n",
        "            df.loc[val_ , \"kfold\"] = int(fold)\n",
        "\n",
        "        df[\"kfold\"] = df[\"kfold\"].astype(int)\n",
        "        return df\n",
        "\n",
        "    def get_test_df(self):\n",
        "        df = pd.read_csv(self.config[\"test_df\"])\n",
        "        if self.config[\"text_encode\"]:\n",
        "            df['full_text'] = df['full_text'].apply(lambda x : self.resolve_encodings_and_normalize(x))\n",
        "        df[\"text_length\"] = df.full_text.apply(lambda x: len(x.split()))\n",
        "        return df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## src/awp.py"
      ],
      "metadata": {
        "id": "aIi0Cke8ggXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {srcdir}/awp.py\n",
        "import gc\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#loss backwardの後に入れて調整する\n",
        "#パラメータ調整に関しては adv_param adv_lr adv_eps adv_step eval_thなどで調整する\n",
        "class AWP:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    adv_param (str): layernameを書く\n",
        "    adv_lr (float): このパラメータは、最初の層の埋め込みのみを攻撃する場合、すべてのパラメータで 0.1に調整されます。\n",
        "    adv_eps (float): パラメーターの動きの最大幅の制限、一般に（0,1）の間で設定\n",
        "    start_epoch (int): 動き始めるエポック\n",
        "    adv_step (int): 攻撃回数、通常1回の攻撃で比較的効果はあるが、複数回の攻撃には正確な adv_lr が必要\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        optimizer,\n",
        "        adv_param=\"weight\",\n",
        "        adv_lr=0.0005,\n",
        "        adv_eps=0.001,\n",
        "        start_epoch=0,\n",
        "        adv_step=1,\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.adv_param = adv_param\n",
        "        self.adv_lr = adv_lr\n",
        "        self.adv_eps = adv_eps\n",
        "        self.start_epoch = start_epoch\n",
        "        self.adv_step = adv_step\n",
        "        self.backup = {}\n",
        "        self.backup_eps = {}\n",
        "\n",
        "    def attack_backward(self, data):\n",
        "        # 開始条件が満たされたときに敵対的訓練を開始する\n",
        "        if (self.adv_lr == 0):\n",
        "            return None\n",
        "\n",
        "        self._save()  # 攻撃のパラメーターの重みを保存する\n",
        "        for i in range(self.adv_step):\n",
        "            self._attack_step()\n",
        "            adv_loss = self.model(**data)\n",
        "            self.optimizer.zero_grad()\n",
        "            adv_loss.backward()\n",
        "            \n",
        "        self._restore()  # 埋め込みパラメーターの復元\n",
        "\n",
        "    def _attack_step(self):\n",
        "        e = 1e-6  # 定义一个极小值\n",
        "        # emb_name パラメータは、モデルの埋め込みのパラメータ名に置き換える必要があります\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
        "                norm1 = torch.norm(param.grad)\n",
        "                norm2 = torch.norm(param.data.detach())\n",
        "                if norm1 != 0 and not torch.isnan(norm1):\n",
        "                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n",
        "                    param.data.add_(r_at)\n",
        "                    param.data = torch.min(\n",
        "                        torch.max(param.data, self.backup_eps[name][0]), self.backup_eps[name][1]\n",
        "                    )\n",
        "                # param.data.clamp_(*self.backup_eps[name])\n",
        "\n",
        "    def _save(self):\n",
        "        # emb_name パラメータは、モデルの埋め込みのパラメータ名に置き換える必要があります\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
        "                # 元のパラメータを保存\n",
        "                if name not in self.backup:\n",
        "                    self.backup[name] = param.data.clone()\n",
        "                    grad_eps = self.adv_eps * param.abs().detach()\n",
        "                    self.backup_eps[name] = (\n",
        "                        self.backup[name] - grad_eps,\n",
        "                        self.backup[name] + grad_eps,\n",
        "                    )\n",
        "\n",
        "    def _restore(self,):\n",
        "        # emb_name パラメータは、モデルの埋め込みのパラメータ名に置き換える必要があります\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if name in self.backup:\n",
        "                param.data = self.backup[name]\n",
        "        self.backup = {}\n",
        "        self.backup_eps = {}"
      ],
      "metadata": {
        "id": "9sDg69lrggK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5h4PxWqNUeW"
      },
      "source": [
        "## src/trainer.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKZ81NjeNV-t"
      },
      "outputs": [],
      "source": [
        "%%writefile {srcdir}/trainer.py\n",
        "import torch\n",
        "from torch.optim import lr_scheduler\n",
        "from transformers import AdamW\n",
        "import os, gc, copy, time, random, string, joblib, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import wandb\n",
        "from .train_datasets import FeedBackDataset, Collate\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from .models import FeedBackModel\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "def set_seed(seed=42):\n",
        "        '''Sets the seed of the entire notebook so results are the same every time we run.\n",
        "        This is for REPRODUCIBILITY.'''\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        # When running on the CuDNN backend, two further options must be set\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        # Set a fixed value for the hash seed\n",
        "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "class Trainers:\n",
        "\n",
        "    def __init__(self, df, config, sweep=False, opt_params=None):\n",
        "        self.config = config\n",
        "        set_seed(config[\"seed\"])\n",
        "        self.df = df\n",
        "        self.device = self.config[\"device\"]\n",
        "        self.fold = self.config[\"fold\"]\n",
        "        self.sweep = sweep\n",
        "        if sweep:\n",
        "            self.sweep_config = opt_params\n",
        "            print(\"start sweep mode\")\n",
        "        else:\n",
        "            wandb.init(\n",
        "                    project=\"feedback3\", \n",
        "                    group=\"baseline\",\n",
        "                    config = opt_params\n",
        "            )\n",
        "        self.opt_params = opt_params\n",
        "\n",
        "    def fetch_scheduler(self):\n",
        "        if self.config['scheduler'] == 'CosineAnnealingLR':\n",
        "            scheduler = lr_scheduler.CosineAnnealingLR(self.optimizer,T_max=self.config['T_max'], \n",
        "                                                    eta_min=self.config['min_lr'])\n",
        "        elif self.config['scheduler'] == 'CosineAnnealingWarmRestarts':\n",
        "            scheduler = lr_scheduler.CosineAnnealingWarmRestarts(self.optimizer,T_0=self.config['T_0'], \n",
        "                                                                eta_min=self.config['min_lr'])\n",
        "        elif self.config['scheduler'] == None:\n",
        "            return None\n",
        "            \n",
        "        return scheduler\n",
        "    \n",
        "    def metric_fn(self, outputs, targets):\n",
        "        colwise_mse = np.mean(np.square(targets - outputs), axis=0)\n",
        "        loss = np.mean(np.sqrt(colwise_mse), axis=0)\n",
        "        return loss\n",
        "\n",
        "    def predict_fn(self, model, test_loader):\n",
        "        model.eval()\n",
        "        \n",
        "        preds = []\n",
        "        text_ids = []\n",
        "        embs = []\n",
        "        for step, data in enumerate(self.valid_loader):\n",
        "            text_id = data['text_id']\n",
        "            ids = data['ids'].to(self.device, dtype = torch.long)\n",
        "            mask = data['mask'].to(self.device, dtype = torch.long)\n",
        "            \n",
        "            with autocast(enabled=True):\n",
        "                outputs, emb = model.get_emb(ids, mask)\n",
        "            preds.append(outputs.cpu().detach().numpy())\n",
        "            embs.append(emb.cpu().detach().numpy())\n",
        "            text_ids.append(text_id)\n",
        "        \n",
        "        preds = np.concatenate(preds)\n",
        "        embs =  np.concatenate(embs)\n",
        "        text_ids = np.concatenate(text_ids)\n",
        "        gc.collect()\n",
        "        pred_df = pd.DataFrame([text_ids,preds],index_col=[\"text_id\",\"pred\"]).T\n",
        "        return pred_df\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def valid_fn(self):\n",
        "        self.model.eval()\n",
        "        \n",
        "        dataset_size = 0\n",
        "        running_loss = 0.0\n",
        "        \n",
        "        TEXT_IDS = []\n",
        "        PREDS = []\n",
        "        EMBS = []\n",
        "        TARGETS = []\n",
        "\n",
        "        for step, data in enumerate(self.valid_loader):\n",
        "            text_id = data['text_id']\n",
        "            ids = data['ids'].to(self.device, dtype = torch.long)\n",
        "            mask = data['mask'].to(self.device, dtype = torch.long)\n",
        "            targets = data['target'].to(self.device, dtype = torch.float)\n",
        "            \n",
        "            with autocast(enabled=False):\n",
        "                outputs, emb = self.model.get_emb(ids, mask)\n",
        "            TEXT_IDS.append(text_id)\n",
        "            PREDS.append(outputs.cpu().detach().numpy())\n",
        "            EMBS.append(emb.cpu().detach().numpy())\n",
        "            TARGETS.append(targets.cpu().detach().numpy())\n",
        "        \n",
        "        TEXT_IDS = np.concatenate(TEXT_IDS)\n",
        "        PREDS = np.concatenate(PREDS)\n",
        "        EMBS = np.concatenate(EMBS)\n",
        "        TARGETS = np.concatenate(TARGETS)\n",
        "        valid_loss = self.metric_fn(PREDS, TARGETS)\n",
        "        print(\"mcrmse score:\",valid_loss)\n",
        "        gc.collect()\n",
        "        labels = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar','conventions']\n",
        "        oof_df = pd.DataFrame([TEXT_IDS],index=[\"text_id\"]).T\n",
        "        for index,label in enumerate(labels):\n",
        "            oof_df[f\"{label}_pred\"] = PREDS[:,index]\n",
        "            oof_df[label] = TARGETS[:,index]\n",
        "        for emb_index in range(EMBS.shape[1]):\n",
        "            oof_df[f\"emb_{emb_index}\"] = EMBS[:,emb_index]\n",
        "        \n",
        "        return valid_loss, oof_df\n",
        "  \n",
        "    def train_one_epoch(self, epoch, best_epoch_loss):\n",
        "        self.model.train()\n",
        "      \n",
        "        dataset_size = 0\n",
        "        running_loss = 0.0\n",
        "\n",
        "        bar = tqdm(enumerate(self.train_loader), total=len(self.train_loader))\n",
        "        self.scaler = GradScaler()\n",
        "        for step, data in bar:\n",
        "            text_id = data['text_id']\n",
        "            data['ids'] = data['ids'].to(self.device, dtype = torch.long)\n",
        "            data['mask'] = data['mask'].to(self.device, dtype = torch.long)\n",
        "            data['target'] = data['target'].to(self.device, dtype = torch.float)\n",
        "\n",
        "            with autocast():\n",
        "                loss = self.model(**data)\n",
        "                loss = loss / self.config['n_accumulate']\n",
        "            self.scaler.scale(loss).backward()\n",
        "\n",
        "            if (step + 1) % self.config['n_accumulate'] == 0:\n",
        "                if self.config[\"max_norm\"] > 0:\n",
        "                    self.scaler.unscale_(self.optimizer)\n",
        "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config[\"max_norm\"])\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "                if self.scheduler is not None:\n",
        "                    self.scheduler.step()\n",
        "                \n",
        "                # zero the parameter gradients\n",
        "                self.optimizer.zero_grad()\n",
        "     \n",
        "            running_loss += (loss.item() * self.config[\"train_batch_size\"]) * self.config[\"n_accumulate\"]\n",
        "            dataset_size += self.config[\"train_batch_size\"]\n",
        "            \n",
        "            epoch_loss = running_loss / dataset_size\n",
        "            \n",
        "            bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n",
        "                            bb_LR=self.optimizer.param_groups[0]['lr'])\n",
        "            \n",
        "            if (step % int(self.config[\"eval_step\"]//self.config[\"train_batch_size\"])==0)and((epoch-1)*len(self.train_loader) + step > int(self.config[\"eval_start\"]//self.config[\"train_batch_size\"])):\n",
        "                val_epoch_loss, self.oof_df = self.valid_fn()\n",
        "                self.model.train()\n",
        "\n",
        "                # deep copy the model\n",
        "                if val_epoch_loss <= best_epoch_loss:\n",
        "                    print(f\"Validation Loss Improved ({best_epoch_loss} ---> {val_epoch_loss})\")\n",
        "                    best_epoch_loss = val_epoch_loss\n",
        "                    best_model = copy.deepcopy(self.model)\n",
        "                    best_model.model.half()\n",
        "                    self.best_model_wts = best_model.state_dict()\n",
        "                    PATH = f\"{self.config['savedir']}/Loss-Fold{self.fold}.bin\"\n",
        "                    torch.save(self.best_model_wts, PATH)\n",
        "                    oof_path = f\"{self.config['savedir']}/oof-Fold{self.fold}.csv\"\n",
        "                    self.oof_df.to_csv(oof_path,index=False)\n",
        "                    # Save a model file from the current directory\n",
        "                    print(f\"Model and oof dataframe Saved\")\n",
        "                    print()\n",
        "                \n",
        "                wandb.log({\n",
        "                    \"valid_loss\":val_epoch_loss,\n",
        "                    \"best_valid_loss\":best_epoch_loss,\n",
        "                    \"train_loss\":epoch_loss\n",
        "                })\n",
        "\n",
        "\n",
        "            \n",
        "        gc.collect()\n",
        "        \n",
        "        return epoch_loss, best_epoch_loss\n",
        "\n",
        "    def run_training(self):\n",
        "        if self.sweep:\n",
        "            wandb.init()\n",
        "            self.config.update(wandb.config)\n",
        "        \n",
        "\n",
        "        #import model and tokenzier\n",
        "        self.model = FeedBackModel(self.config['model_name'], self.config)\n",
        "        self.model.to(self.config['device'])\n",
        "        self.config[\"tokenizer\"] = AutoTokenizer.from_pretrained(self.config['model_name'], use_fast=True)\n",
        "        if self.config[\"add_bntoken\"]:\n",
        "            self.config[\"tokenizer\"].add_tokens([\"\\n\"], special_tokens=True)\n",
        "        self.config[\"tokenizer\"].save_pretrained(f\"{self.config['savedir']}/tokenizer_fold{self.config['fold']}\")\n",
        "\n",
        "        self.num_epochs = self.config[\"epochs\"]\n",
        "        #optimizer setting\n",
        "        param_optimizer = list(self.model.model.named_parameters())\n",
        "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': self.config['weight_decay']},\n",
        "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
        "            {'params': [p for n, p in self.model.fc.named_parameters()], 'learning rate': self.config['head_lr'],'weight_decay': 0.0}\n",
        "        ]\n",
        "\n",
        "        self.optimizer = AdamW(\n",
        "            optimizer_grouped_parameters,\n",
        "            lr=self.config['learning_rate']\n",
        "        )\n",
        "        self.scheduler = self.fetch_scheduler()\n",
        "\n",
        "        collate_fn = Collate(self.config)\n",
        "        df_train = self.df[self.df.kfold != self.fold].reset_index(drop=True)\n",
        "        train_dataset = FeedBackDataset(df_train, config=self.config)\n",
        "        self.train_loader = DataLoader(\n",
        "            train_dataset, \n",
        "            batch_size=self.config['train_batch_size'], \n",
        "            collate_fn = collate_fn, \n",
        "            num_workers=os.cpu_count(), \n",
        "            pin_memory=True, \n",
        "            shuffle=True,\n",
        "            drop_last=True,\n",
        "        )\n",
        "\n",
        "        #validate set\n",
        "        collate_fn.dropout_drop = 0\n",
        "        df_valid = self.df[self.df.kfold == self.fold].sort_values(\"text_length\").reset_index(drop=True)\n",
        "        valid_dataset = FeedBackDataset(df_valid, config=self.config)\n",
        "        self.valid_loader = DataLoader(\n",
        "            valid_dataset, \n",
        "            batch_size=self.config['valid_batch_size'], \n",
        "            collate_fn = collate_fn, \n",
        "            num_workers=os.cpu_count(), \n",
        "            pin_memory=True, \n",
        "            shuffle=False,\n",
        "            drop_last=False,\n",
        "        )\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n",
        "        \n",
        "        start = time.time()\n",
        "        best_model_wts = copy.deepcopy(self.model.state_dict())\n",
        "        if self.config[\"seed_average\"]:\n",
        "            pass\n",
        "        else:\n",
        "            self.best_epoch_loss = np.inf\n",
        "        val_epoch_loss = self.valid_fn()\n",
        "        for epoch in range(1, self.num_epochs + 1): \n",
        "            gc.collect()\n",
        "            train_epoch_loss, self.best_epoch_loss, best_model_wts, oof_df = self.train_one_epoch(epoch=epoch, best_epoch_loss=self.best_epoch_loss)\n",
        "        \n",
        "        end = time.time()\n",
        "        time_elapsed = end - start\n",
        "        print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n",
        "            time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n",
        "        # save only best model for drive\n",
        "        PATH = f\"{self.config['savedir_drive']}/Loss-Fold{self.fold}_{best_epoch_loss}.bin\"\n",
        "        torch.save(self.best_model_wts, PATH)\n",
        "        oof_path = f\"{self.config['savedir_drive']}/oof-Fold{self.fold}_{best_epoch_loss}.csv\"\n",
        "        self.oof_df.to_csv(oof_path,index=False)\n",
        "        \n",
        "        print(\"Best Loss: {:.4f}\".format(self.best_epoch_loss))\n",
        "    \n",
        "    def run_seed_average(self):\n",
        "        #reset best loss\n",
        "        self.best_epoch_loss = np.inf\n",
        "        for seed in [42,441,3031]:\n",
        "            set_seed(seed)\n",
        "            self.run_training()\n",
        "\n",
        "    def run_sweep(self,sweep_id=None):\n",
        "        if sweep_id is None:\n",
        "            sweep_id = wandb.sweep(self.sweep_config)\n",
        "        if self.config[\"seed_average\"]:\n",
        "            wandb.agent(sweep_id, self.run_seed_average)\n",
        "        else:\n",
        "            wandb.agent(sweep_id, self.run_training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nd50N9pdLUTC"
      },
      "source": [
        "## predict.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FX_WrazLT5P"
      },
      "outputs": [],
      "source": [
        "%%writefile {savedir}/predict.py\n",
        "\n",
        "import torch\n",
        "from torch.optim import lr_scheduler\n",
        "from transformers import AdamW\n",
        "import os, gc, copy, time, random, string, joblib, sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import json\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "import argparse\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--dataset_dir')\n",
        "parser.add_argument('--config_path')\n",
        "parser.add_argument('--testdf_path')\n",
        "parser.add_argument('--model_name_or_path')\n",
        "parser.add_argument('--model_weight_path')\n",
        "parser.add_argument('--batch_size')\n",
        "parser.add_argument('--output_path')\n",
        "args = parser.parse_args()\n",
        "\n",
        "dataset_dir = args.dataset_dir\n",
        "config_path = args.config_path\n",
        "testdf_path = args.testdf_path\n",
        "model_path =  args.model_name_or_path\n",
        "weight_path = args.model_weight_path\n",
        "batch_size = int(args.batch_size)\n",
        "output_path = args.output_path\n",
        "\n",
        "with open(config_path) as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "config = config[\"fold0\"]\n",
        "config[\"test_df\"] = testdf_path\n",
        "config[\"model_name\"] = model_path\n",
        "config[\"weight_path\"] = weight_path\n",
        "config[\"test_batch_size\"] = batch_size\n",
        "\n",
        "\n",
        "sys.path.append(dataset_dir)\n",
        "#sys.path.append(\"/kaggle/input/iterative-stratification/iterative-stratification-master\")\n",
        "import src\n",
        "from src.models import FeedBackModel\n",
        "from src.test_datasets import testCollate, feedbacktestDataset\n",
        "from src.train_datasets import FeedBackDataset, Collate\n",
        "\n",
        "device = \"cuda\"\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig, AdamW, AutoModelForMaskedLM\n",
        "\n",
        "config[\"tokenizer\"] = AutoTokenizer.from_pretrained(config['model_name'], use_fast=True)\n",
        "\n",
        "hlc = src.helper(config=config)\n",
        "test = hlc.get_test_df()\n",
        "\n",
        "collate_fn = testCollate(config)\n",
        "test_dataset = feedbacktestDataset(test, config=config)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, \n",
        "    batch_size=config[\"test_batch_size\"], \n",
        "    collate_fn = collate_fn, \n",
        "    num_workers=os.cpu_count(), \n",
        "    pin_memory=True, \n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        ")\n",
        "\n",
        "model = FeedBackModel(config[\"model_name\"]).to(device)\n",
        "model.load_state_dict(torch.load(config[\"weight_path\"]))\n",
        "model.model.half()\n",
        "model.eval()\n",
        "\n",
        "preds = []\n",
        "text_ids = []\n",
        "embs = []\n",
        "bar = tqdm(enumerate(test_loader), total=len(test_loader))\n",
        "for step, data in bar:\n",
        "    text_id = data['text_id']\n",
        "    ids = data['ids'].to(device, dtype = torch.long)\n",
        "    mask = data['mask'].to(device, dtype = torch.long)\n",
        "    with torch.no_grad():\n",
        "        outputs, emb = model.get_emb(ids, mask)\n",
        "    preds.append(outputs.cpu().detach().numpy())\n",
        "    embs.append(emb.cpu().detach().numpy())\n",
        "    text_ids.append(text_id)\n",
        "\n",
        "preds = np.concatenate(preds)\n",
        "embs = np.concatenate(embs)\n",
        "text_ids = np.concatenate(text_ids)\n",
        "gc.collect()\n",
        "labels = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar','conventions']\n",
        "pred_df = pd.DataFrame([text_ids],index=[\"text_id\"]).T\n",
        "for index,label in enumerate(labels):\n",
        "    pred_df[label] = preds[:,index]\n",
        "for emb_index in range(EMBS.shape[1]):\n",
        "    pred_df[f\"emb_{emb_index}\"] = EMBS[:,emb_index]\n",
        "\n",
        "pred_df.to_csv(output_path,index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcA8rLKHJFcc"
      },
      "source": [
        "## train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0z4H5EvHJAoF"
      },
      "outputs": [],
      "source": [
        "%%writefile {savedir}/train.py\n",
        "\n",
        "import sys, os, json\n",
        "\n",
        "# For descriptive error messages\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import os, gc, copy, time, random, string, joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "tqdm.pandas()\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Utils\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "# For Transformer Models\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig, AdamW, AutoModelForMaskedLM\n",
        "\n",
        "import wandb\n",
        "wandb.login(key=\"dd1758beb9fd6044fdc028dfc9245bba1c869a29\")\n",
        "\n",
        "import argparse\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--savedir')\n",
        "parser.add_argument('--savedir_drive')\n",
        "parser.add_argument('--root_dir')\n",
        "parser.add_argument('--train_fold', nargs='*')\n",
        "args = parser.parse_args()\n",
        "savedir = args.savedir\n",
        "savedir_drive = args.savedir_drive\n",
        "rootdir = args.root_dir\n",
        "train_fold = np.array(args.train_fold ,dtype=np.int64)\n",
        "print(train_fold)\n",
        "\n",
        "with open(f'{savedir}/trainparam.json') as f:\n",
        "    CONFIG = json.load(f)\n",
        "\n",
        "sys.path.append(savedir)\n",
        "import src\n",
        "\n",
        "for fold in train_fold:\n",
        "    print(f\"====== Fold: {fold} ======\")\n",
        "    config = CONFIG[f\"fold{fold}\"]\n",
        "    config[\"savedir_drive\"] = savedir_drive \n",
        "    hlc = src.helper(config=config)\n",
        "    df = hlc.get_df()\n",
        "    hlc.config[\"savedir\"] = savedir\n",
        "    hlc.config[\"device\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # Start a wandb run\n",
        "    hlc.config[\"fold\"] = fold \n",
        "    trainer = src.Trainers(\n",
        "        df = df,\n",
        "        config = hlc.config,\n",
        "    )\n",
        "    if hlc.config[\"seed_average\"]:\n",
        "        trainer.run_seed_average()\n",
        "    else:\n",
        "        trainer.run_training()\n",
        "    \n",
        "    del trainer\n",
        "    _ = gc.collect()\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rFIJkkK9xOu"
      },
      "source": [
        "## sweep.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAezOoXH9w43"
      },
      "outputs": [],
      "source": [
        "%%writefile {savedir}/sweep.py\n",
        "\n",
        "import sys, os, json\n",
        "\n",
        "# For descriptive error messages\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import os, gc, copy, time, random, string, joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "tqdm.pandas()\n",
        "from textblob import TextBlob\n",
        "\n",
        "import wandb\n",
        "wandb.login(key=\"dd1758beb9fd6044fdc028dfc9245bba1c869a29\")\n",
        "\n",
        "# Utils\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "# For Transformer Models\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig, AdamW, AutoModelForMaskedLM\n",
        "\n",
        "import argparse\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--version')\n",
        "parser.add_argument('--savedir')\n",
        "parser.add_argument('--savedir_drive')\n",
        "parser.add_argument('--root_dir')\n",
        "parser.add_argument('--fold')\n",
        "parser.add_argument('--resume', action='store_true')\n",
        "args = parser.parse_args()\n",
        "\n",
        "version = args.version\n",
        "savedir = args.savedir\n",
        "savedir_drive = args.savedir_drive\n",
        "rootdir = args.root_dir\n",
        "fold = int(args.fold)\n",
        "resume = args.resume\n",
        "\n",
        "with open(f'{savedir}/trainparam.json') as f:\n",
        "    CONFIG = json.load(f)\n",
        "\n",
        "with open(f'{savedir}/opt_parameters.json') as f:\n",
        "    opt_params = json.load(f)\n",
        "\n",
        "CONFIG = CONFIG[f\"fold{fold}\"]\n",
        "\n",
        "opt_params[\"name\"] = f\"{version}-fold{fold}\"\n",
        "\n",
        "CONFIG[\"savedir\"] = savedir\n",
        "CONFIG[\"savedir_drive\"] = savedir_drive\n",
        "CONFIG[\"version\"] = version\n",
        "sys.path.append(CONFIG[\"savedir\"])\n",
        "CONFIG[\"device\"] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "import src\n",
        "hlc = src.helper(config=CONFIG)\n",
        "df = hlc.get_df()\n",
        "\n",
        "hlc.config[\"fold\"] = fold\n",
        "hlc.config[\"exp\"] = f\"{version}\"\n",
        "print(f\"====== sweep mode Fold: {fold} ======\")\n",
        "    \n",
        "trainer = src.Trainers(\n",
        "    df = df,\n",
        "    config = hlc.config,\n",
        "    opt_params = opt_params,\n",
        "    sweep = True\n",
        ")\n",
        "if resume:\n",
        "    trainer.run_sweep(sweep_id=CONFIG[\"sweep_id\"])\n",
        "else:\n",
        "    trainer.run_sweep()\n",
        "del trainer\n",
        "_ = gc.collect()\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il3DjX_rcDQk"
      },
      "source": [
        "## trainparam.json\n",
        "#### best fold seed:539(best loss), 846, 669, 518\n",
        "#### mlm data /content/drive/MyDrive/results_for_furu/mlm/{model_name} で使用可能(日付管理で更新する？？)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tv8BZ3lacFUE"
      },
      "outputs": [],
      "source": [
        "%%writefile {savedir}/trainparam.json\n",
        "\n",
        "{\n",
        "    \"fold0\":{\n",
        "        \"seed\": 431,\n",
        "        \"fold_seed\":518,\n",
        "        \"T_max\": 278,\n",
        "        \"add_bntoken\":1,\n",
        "        \"backbone_dropout_prob\":0,\n",
        "        \"epochs\": 6,\n",
        "        \"freeze_emb\":1,\n",
        "        \"freeze_layernum\":1,\n",
        "        \"head_dropout_prob\":0.1,\n",
        "        \"head_lr\":0.0049046,\n",
        "        \"init_head\":1,\n",
        "        \"layer_norm_eps\":1e-7,\n",
        "        \"learning_rate\": 0.00005558104,\n",
        "        \"max_length\": 817,\n",
        "        \"max_norm\":1,\n",
        "        \"min_lr\": 1.6066978e-7,\n",
        "        \"model_name\": \"microsoft/deberta-v3-large\",\n",
        "        \"n_accumulate\": 8,\n",
        "        \"reinit_layernum\":1,\n",
        "        \"stable_prob1\":0,\n",
        "        \"stable_prob2\":0,\n",
        "        \"stable_prob3\":0.1,\n",
        "        \"stable_prob4\":0.1,\n",
        "        \"stable_prob5\":0.1,\n",
        "        \"text_encode\":1,\n",
        "        \"token_dropout_prob\":0.15,\n",
        "        \"token_dropout_ratio\":0.2,\n",
        "        \"train_batch_size\": 4,\n",
        "        \"weight_decay\": 0,\n",
        "        \"exp_name\":\"baseline\",\n",
        "        \"exp\":0,\n",
        "        \"valid_batch_size\": 2,\n",
        "        \"eval_step\":600,\n",
        "        \"eval_start\":3000,\n",
        "        \"scheduler\": \"CosineAnnealingLR\",\n",
        "        \"n_fold\": 4,\n",
        "        \"competition\": \"FeedBack3\",\n",
        "        \"train_df\": \"/kaggle/train.csv\",\n",
        "        \"test_df\":\"/kaggle/test.csv\",\n",
        "        \"sweep_id\":\"furufuru/feedback3/ksm2elr7\",\n",
        "        \"seed_average\":false\n",
        "    },\n",
        "    \"fold1\":{\n",
        "        \"seed\": 431,\n",
        "        \"fold_seed\":518,\n",
        "        \"T_max\": 347,\n",
        "        \"add_bntoken\":1,\n",
        "        \"backbone_dropout_prob\":0,\n",
        "        \"epochs\": 6,\n",
        "        \"freeze_emb\":0,\n",
        "        \"freeze_layernum\":1,\n",
        "        \"head_dropout_prob\":0,\n",
        "        \"head_lr\":0.004640476,\n",
        "        \"init_head\":1,\n",
        "        \"layer_norm_eps\":1e-7,\n",
        "        \"learning_rate\": 0.0000235214,\n",
        "        \"max_length\": 1254,\n",
        "        \"max_norm\":40,\n",
        "        \"min_lr\": 8.521333e-7,\n",
        "        \"model_name\": \"microsoft/deberta-v3-large\",\n",
        "        \"n_accumulate\": 8,\n",
        "        \"reinit_layernum\":1,\n",
        "        \"stable_prob1\":0.05,\n",
        "        \"stable_prob2\":0,\n",
        "        \"stable_prob3\":0.1,\n",
        "        \"stable_prob4\":0.05,\n",
        "        \"stable_prob5\":0,\n",
        "        \"text_encode\":1,\n",
        "        \"token_dropout_prob\":0.2,\n",
        "        \"token_dropout_ratio\":0.2,\n",
        "        \"train_batch_size\": 4,\n",
        "        \"weight_decay\": 0,\n",
        "        \"exp_name\":\"baseline\",\n",
        "        \"exp\":0,\n",
        "        \"valid_batch_size\": 2,\n",
        "        \"eval_step\":600,\n",
        "        \"eval_start\":3000,\n",
        "        \"scheduler\": \"CosineAnnealingLR\",\n",
        "        \"n_fold\": 4,\n",
        "        \"competition\": \"FeedBack3\",\n",
        "        \"train_df\": \"/kaggle/train.csv\",\n",
        "        \"test_df\":\"/kaggle/test.csv\",\n",
        "        \"sweep_id\":\"furufuru/feedback3/zcu7lh4t\",\n",
        "        \"seed_average\": false    \n",
        "    },\n",
        "    \"fold2\":{\n",
        "        \"seed\": 431,\n",
        "        \"fold_seed\":518,\n",
        "        \"T_max\": 350,\n",
        "        \"add_bntoken\":1,\n",
        "        \"backbone_dropout_prob\":0,\n",
        "        \"epochs\": 6,\n",
        "        \"freeze_emb\":1,\n",
        "        \"freeze_layernum\":1,\n",
        "        \"head_dropout_prob\":0,\n",
        "        \"head_lr\":0.003607202,\n",
        "        \"init_head\":1,\n",
        "        \"layer_norm_eps\":1e-7,\n",
        "        \"learning_rate\": 0.00001366467,\n",
        "        \"max_length\": 1157,\n",
        "        \"max_norm\":100,\n",
        "        \"min_lr\": 4.0966938e-7,\n",
        "        \"model_name\": \"microsoft/deberta-v3-large\",\n",
        "        \"n_accumulate\": 11,\n",
        "        \"reinit_layernum\":1,\n",
        "        \"stable_prob1\":0.05,\n",
        "        \"stable_prob2\":0.1,\n",
        "        \"stable_prob3\":0.1,\n",
        "        \"stable_prob4\":0.1,\n",
        "        \"stable_prob5\":0.1,\n",
        "        \"text_encode\":1,\n",
        "        \"token_dropout_prob\":0.05,\n",
        "        \"token_dropout_ratio\":0.05,\n",
        "        \"train_batch_size\": 4,\n",
        "        \"weight_decay\": 0,\n",
        "        \"exp_name\":\"baseline\",\n",
        "        \"exp\":0,\n",
        "        \"valid_batch_size\": 2,\n",
        "        \"eval_step\":600,\n",
        "        \"eval_start\":3000,\n",
        "        \"scheduler\": \"CosineAnnealingLR\",\n",
        "        \"n_fold\": 4,\n",
        "        \"competition\": \"FeedBack3\",\n",
        "        \"train_df\": \"/kaggle/train.csv\",\n",
        "        \"test_df\":\"/kaggle/test.csv\",\n",
        "        \"sweep_id\":\"furufuru/feedback3/1jfytjdi\",\n",
        "        \"seed_average\": true    \n",
        "    },\n",
        "    \"fold3\":{\n",
        "        \"seed\": 431,\n",
        "        \"fold_seed\":518,\n",
        "        \"T_max\": 301,\n",
        "        \"add_bntoken\":1,\n",
        "        \"backbone_dropout_prob\":0,\n",
        "        \"epochs\": 5,\n",
        "        \"freeze_emb\":1,\n",
        "        \"freeze_layernum\":6,\n",
        "        \"head_dropout_prob\":0,\n",
        "        \"head_lr\":0.00424512,\n",
        "        \"init_head\":1,\n",
        "        \"layer_norm_eps\":1e-7,\n",
        "        \"learning_rate\": 0.00003627028,\n",
        "        \"max_length\": 942,\n",
        "        \"max_norm\":1,\n",
        "        \"min_lr\": 7.43945e-7,\n",
        "        \"model_name\": \"microsoft/deberta-v3-large\",\n",
        "        \"n_accumulate\": 5,\n",
        "        \"reinit_layernum\":2,\n",
        "        \"stable_prob1\":0.05,\n",
        "        \"stable_prob2\":0.1,\n",
        "        \"stable_prob3\":0.1,\n",
        "        \"stable_prob4\":0.05,\n",
        "        \"stable_prob5\":0.1,\n",
        "        \"text_encode\":0,\n",
        "        \"token_dropout_prob\":0.2,\n",
        "        \"token_dropout_ratio\":0.05,\n",
        "        \"train_batch_size\": 4,\n",
        "        \"weight_decay\": 0,\n",
        "        \"exp_name\":\"baseline\",\n",
        "        \"exp\":0,\n",
        "        \"valid_batch_size\": 2,\n",
        "        \"eval_step\":600,\n",
        "        \"eval_start\":3000,\n",
        "        \"scheduler\": \"CosineAnnealingLR\",\n",
        "        \"n_fold\": 4,\n",
        "        \"competition\": \"FeedBack3\",\n",
        "        \"train_df\": \"/kaggle/train.csv\",\n",
        "        \"test_df\":\"/kaggle/test.csv\",\n",
        "        \"sweep_id\":\"furufuru/feedback3/3t36pwbp\",\n",
        "        \"seed_average\": true\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5G3gOBMv8p7O"
      },
      "source": [
        "## opt_parameters.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpOqhVbK6O3i"
      },
      "outputs": [],
      "source": [
        "%%writefile {savedir}/opt_parameters.json\n",
        "{\n",
        "    \"project\":\"feedback3\",\n",
        "    \"method\": \"bayes\",\n",
        "    \"metric\": {\n",
        "        \"name\": \"best_valid_loss\",\n",
        "        \"goal\": \"minimize\"\n",
        "    },\n",
        "    \"early_terminate\":{\n",
        "        \"type\": \"hyperband\",\n",
        "        \"min_iter\": 5\n",
        "    },\n",
        "    \"parameters\":{\n",
        "        \"reinit_layernum\":{\n",
        "            \"values\":[0,1,2,3]\n",
        "        },\n",
        "        \"init_head\":{\n",
        "            \"value\": 1\n",
        "        },\n",
        "        \"text_encode\":{\n",
        "            \"values\":[0,1]\n",
        "        },\n",
        "        \"add_bntoken\":{\n",
        "            \"values\":[0,1]\n",
        "        },\n",
        "        \"freeze_emb\":{\n",
        "            \"values\":[0,1]\n",
        "        },\n",
        "        \"freeze_layernum\":{\n",
        "            \"distribution\":\"int_uniform\",\n",
        "            \"min\":0,\n",
        "            \"max\":6\n",
        "        },\n",
        "        \"max_norm\":{\n",
        "            \"values\":[0,1,10,20,30,40,50,100]\n",
        "        },\n",
        "        \"model_name\":{\n",
        "            \"value\": \"microsoft/deberta-v3-large\"\n",
        "        },\n",
        "        \"epochs\":{\n",
        "            \"values\": [4,5,6]\n",
        "        },\n",
        "        \"train_batch_size\":{\n",
        "            \"value\": 4\n",
        "        },\n",
        "        \"stable_prob1\":{\n",
        "            \"values\": [0,0.05,0.1]\n",
        "        },\n",
        "        \"stable_prob2\":{\n",
        "            \"values\": [0,0.05,0.1]\n",
        "        },\n",
        "        \"stable_prob3\":{\n",
        "            \"values\": [0,0.05,0.1]\n",
        "        },\n",
        "        \"stable_prob4\":{\n",
        "            \"values\": [0,0.05,0.1]\n",
        "        },\n",
        "        \"stable_prob5\":{\n",
        "            \"values\": [0,0.05,0.1]\n",
        "        },\n",
        "        \"token_dropout_ratio\":{\n",
        "            \"values\": [0.05,0.1,0.15,0.2,0.25]\n",
        "        },\n",
        "        \"token_dropout_prob\":{\n",
        "            \"values\": [0,0.05,0.1,0.15,0.2,0.25]\n",
        "        },\n",
        "        \"backbone_dropout_prob\":{\n",
        "            \"value\": 0\n",
        "        },\n",
        "        \"head_dropout_prob\":{\n",
        "            \"values\": [0,0.05,0.1]\n",
        "        },\n",
        "        \"layer_norm_eps\":{\n",
        "            \"value\": 1e-7\n",
        "        },\n",
        "        \"learning_rate\": {\n",
        "            \"distribution\": \"uniform\",\n",
        "            \"min\": 1e-5,\n",
        "            \"max\": 6e-5\n",
        "        },\n",
        "        \"head_lr\":{\n",
        "            \"distribution\": \"uniform\",\n",
        "            \"min\": 1e-3,\n",
        "            \"max\": 5e-3\n",
        "        },\n",
        "        \"weight_decay\":{\n",
        "            \"value\": 0\n",
        "        },\n",
        "        \"max_length\":{\n",
        "            \"distribution\": \"int_uniform\",\n",
        "            \"min\":768,\n",
        "            \"max\":1360\n",
        "        },\n",
        "        \"n_accumulate\":{\n",
        "            \"distribution\": \"int_uniform\",\n",
        "            \"min\":4,\n",
        "            \"max\":20\n",
        "        },\n",
        "        \"T_max\":{\n",
        "            \"distribution\": \"int_uniform\",\n",
        "            \"min\":200,\n",
        "            \"max\":400\n",
        "        },\n",
        "        \"min_lr\":{\n",
        "            \"distribution\": \"uniform\",\n",
        "            \"min\": 1e-7,\n",
        "            \"max\": 1e-6\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXulTsWVxMrl"
      },
      "source": [
        "# kaggle datasetsの作成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LQKxk0uxOZK"
      },
      "outputs": [],
      "source": [
        "%%writefile {savedir}/dataset-metadata.json\n",
        "\n",
        "{\n",
        "  \"licenses\": [\n",
        "    {\n",
        "      \"name\": \"CC0-1.0\"\n",
        "    }\n",
        "  ], \n",
        "  \"id\": \"kunihikofurugori/baseline-4fold\",\n",
        "  \"title\": \"beseline-4fold\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbEi5FxJy0JG"
      },
      "outputs": [],
      "source": [
        "#bash用\n",
        "print(savedir)\n",
        "#!kaggle datasets version -p /kaggle/beaeline -m \"update\" -r zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSKFa_ifxel-"
      },
      "outputs": [],
      "source": [
        "#!kaggle datasets create -p {savedir} -r zip\n",
        "#!kaggle datasets version -p {savedir} -m \"5fold_update\" -r zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx5v9h7x3o1c"
      },
      "source": [
        "# bash実行"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fk7lTAVPUTKk"
      },
      "outputs": [],
      "source": [
        "!python3 {savedir}/train.py --savedir {savedir} --savedir_drive {savedir_drive} --root_dir {ROOT_PATH} --train_fold 2 3\n",
        "#!python3 {savedir}/sweep.py --version {version} --savedir {savedir} --savedir_drive {savedir_drive} --root_dir {ROOT_PATH} --fold 0 --resume"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "private_outputs": true,
      "background_execution": "on",
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}